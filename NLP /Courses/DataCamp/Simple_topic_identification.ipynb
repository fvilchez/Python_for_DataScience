{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a proceder nuestro primer contador de palabras, para ello vamos hacer uso de artículos de wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cargamos los datos \n",
    "file = open('wiki_text_debugging.txt', mode = 'r')\n",
    "info = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 68), ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('debugging', 40)]\n"
     ]
    }
   ],
   "source": [
    "#Tokenizamos nuestro artículo\n",
    "tokens = word_tokenize(info)\n",
    "\n",
    "#Convertimos todos y cada uno de nuestros tokens a miniscula\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "#Nos creamos nuestro objeto tipo Counter\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "#Mostramos los 10 tokens más comunes \n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de trabajar con técnicas de procesado de lenguaje natural es muy conveniente realizar un pre-procesamiento de datos que nos permitirá tener nuestros datos de una forma adecuada, entre las técnicas más comunes se encuentran:\n",
    "\n",
    "* Pasar todo a minúsculas.\n",
    "\n",
    "* Eliminar signos de puntuación etc.\n",
    "\n",
    "* Eliminar palabras sin significado (stopwords).\n",
    "\n",
    "* Lematizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 40), ('system', 25), ('software', 16), ('bug', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('used', 12)]\n"
     ]
    }
   ],
   "source": [
    "#Tokenizamos\n",
    "tokens = word_tokenize(info)\n",
    "\n",
    "#Convertimos todos y cada uno de los tokens a minúscula\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "#Nos quedamos con aquellos tokens alfabéticos\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "#Eliminamos las palabras sin ningún tipo de significado\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "#Lematizamos\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "#Nos creamos el Counter\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "#Mostramos los 10 tokens más comunes \n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and querying a corpus with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim se trata de una herramienta open source muy potente que nos facilita y permite realizar una gran cantidad de operaciones con texto de una forma sencilla y en pocas líneas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#En primer lugar nos vamos a crear una función que nos permita aplicar todo el preprocesamiento visto\n",
    "def preprocesa(texto):\n",
    "    #Tokenizamos\n",
    "    tokens = word_tokenize(texto)\n",
    "    #Convertimos a minúscula\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "    #Eliminamos signos de puntuación etc\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "    #Eliminamos las palabras sin significado\n",
    "    no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "    #Lematizamos\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "    return(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtenemos una lista con los ficheros txt\n",
    "os.chdir(os.getcwd())\n",
    "lista_ficheros = glob.glob(\"*.txt\")\n",
    "\n",
    "#Obtenemos una lista de listas donde cada lista de la lista principal será nuestro texto preprocesado\n",
    "articles = []\n",
    "for fichero in lista_ficheros:\n",
    "    file = open(fichero, mode = 'r')\n",
    "    info = file.read()\n",
    "    articles.append(preprocesa(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (8, 3), (10, 3), (13, 10), (19, 2), (22, 1), (24, 2), (27, 1), (32, 3)]\n"
     ]
    }
   ],
   "source": [
    "#Nos creamos nuestro diccionario\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "#Seleccionamos el id de la palabra computer\n",
    "computer_id = dictionary.token2id.get('computer')\n",
    "\n",
    "#Nos creamos nuestro corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "#Del t artículo mostramos sus 10 primeras palabras y su frencuencia de aparición\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta lista de tuplas indican el id del token y su frecuencia de aparición. Si queremos ver el token podemos hacer uso de **print(dictionary.get(token_id))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos hacer uso de nuestro corpus y diccionario para ver los términos más comunes por documento y entre documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software 160\n",
      "computer 56\n",
      "application 36\n",
      "system 33\n",
      "user 23\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos la información de nuestro cuarto documento\n",
    "doc = corpus[4]\n",
    "\n",
    "#Ordenamos por frecuencia de aparición los tokens de dicho documento\n",
    "bow_doc = sorted(doc, key = lambda w: w[1], reverse = True)\n",
    "\n",
    "#Obtenemos las 5 topics más comunes de nuestro documento\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nos creamos un diccionario que contendrá la frecuencia de aparación de cada uno de nuestros tokens a lo largo de \n",
    "#los 12 documentos\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer 749\n",
      "software 450\n",
      "program 340\n",
      "cite 322\n",
      "language 320\n"
     ]
    }
   ],
   "source": [
    "#Ordenamos por frecuencia de aparición\n",
    "sorted_word_count = sorted(total_word_count.items(), key = lambda w: w[1], reverse = True)\n",
    "\n",
    "#Mostramos las 5 palabras más comunes \n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf with Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tf-idf se trata de una formula que nos permite ponderar la importancia de una palabra en corpus de documentos. Es decir, si estamos intentando extraer los topics de un conjunto de textos que hablan sobre baloncesto, es posible que las palabras como : canasta, baloncesto se repitan mucho pero realmente no están aportando ningún tipo de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.013505646340859068), (1, 0.0028126309499257561), (8, 0.01876503906999049), (10, 0.0084378928497772683), (19, 0.027011292681718136)]\n"
     ]
    }
   ],
   "source": [
    "#Aplicamos la tf-idf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "#Calculamos la tfidf para el documento 5\n",
    "tfidf_weights = tfidf[corpus[4]]\n",
    "\n",
    "#Mostramos los 5 primeros pesos\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patent 0.299404430946\n",
      "apis 0.230004355177\n",
      "license 0.165846262831\n",
      "application 0.159768085234\n",
      "latter 0.153336236785\n"
     ]
    }
   ],
   "source": [
    "#Ordenamos los pesos\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key = lambda w: w[1], reverse = True)\n",
    "\n",
    "#Vemos el top5 \n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary[term_id], weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
