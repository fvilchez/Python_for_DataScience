{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing regular expressions: re.split() and re.findall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a proceder a hacer uso de expresiones regulares para machear dígitos, palabras y caracteres de tipo no alfanuméricos. A la hora de hacer uso de expresiones regulares es muy importante hacer uso del prefijo **r** para asegurarnos que nuestros patrones son interpretados de la forma deseada. De lo contrario es posible que nos encontremos problemas con las secuencas de escape en strings. Por ejemplo, **\"\\n\"** en Python es usado para indicar una nueva línea, pero si hacemos uso del prefijo **r** se interpretará como la cadena **\\n**, es decir el carácter **\"\\\"** seguido del carácter **\"n\"** y no como una nueva línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "#Creamos una expresión regular que nos permita separar la cadena my_string en oraciones. Puesto que cada oración \n",
    "#termina en !,? o ., necesitamos separar por estos carácteres.\n",
    "sentence_endings = r\"[?.!]\"\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "#Creamos una expresión que nos permite encontrar aquellas palabras que tiene mayúsculas\n",
    "capitalize_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalize_words, my_string))\n",
    "\n",
    "#Creamos una expresión que nos permita separar por uno o más espacios en blanco\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "#Creamos una expresión regular que nos permite encontrar todos los dígitos en nuestro texto\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "La tokenización consiste en las transformacion de un string o documento en tokens (que son trozos más pequeños). La librería **nltk** de python dispone de diversas funciones que nos permiten tokenizar de forma adecuada. Entre las que se encuentran:\n",
    "\n",
    "* **sent_tokenize : ** tokeniza un documento en oraciones.\n",
    "\n",
    "* **regexp_tokenize : ** nos permite tokenizar un string o documento a partir de una expresión regular, esto nos permite tener un control más personal sobre la tokenización.\n",
    "\n",
    "* **TweetTokenizer : ** se trata una clase especial para la tokenización de tweets, nos permite separar hastags, menciones etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cargamos los datos \n",
    "file = open(\"grail.txt\", mode = \"r\")\n",
    "info = file.read()\n",
    "file.close()\n",
    "#print(file.read())\n",
    "\n",
    "#Tokenizamos nuestro objeto tipo file\n",
    "sentences = sent_tokenize(info)\n",
    "\n",
    "#Hacemos uso de word_tokenize para tokenizar la cuarta frase\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "#Encontramos los tokens unicos asociados al texto completo \n",
    "unique_toquens = set(word_tokenize(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More regex with re.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a proceder a hacer uso de **re.search()** y **re.match()** para encontrar tokens específicos en un string o documento. La diferencia entre estas dos funciones es que **re.search()** nos permite encontrar un patrón determinado en un string/documento mediante una expresión regular, sin embargo, **re.match()** lo que nos permite es ver si un determinado patrón determinado por una expresión regular coincide con un string/documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n"
     ]
    }
   ],
   "source": [
    "#Hacemos uso de re.search() para encontrar la palabra coconuts\n",
    "match = re.search('coconuts', info)\n",
    "\n",
    "#Vemos en los índices de nuestro texto en los que hemos encontrado la palabra\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "source": [
    "#Nos creamos una expresión regular que nos permite encontrar cualquier cosa que es encuentre entre corchetes\n",
    "pattern1 = r'\\[.*\\]'\n",
    "\n",
    "#Hacemos uso de re.search para encontrar la primera coincidencia\n",
    "print(re.search(pattern1, info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "#NOs creamos un patrón que sea capaz de detectar cosas como NOMBRE : \n",
    "pattern = r'[\\w\\s]+:'\n",
    "\n",
    "#Hacemos uso del match para encontrar la primera coincidencia en nuestro string\n",
    "print(re.match(pattern, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos un string tal como el siguiente: \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\". Queremos tokenizar este string, manteniendo los signos de puntuación, además queremos que #1 sea un token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
     ]
    }
   ],
   "source": [
    "#Nos definimos el string\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "#Nos creamos nuestra expresión regular\n",
    "print(regexp_tokenize(my_string, r'\\w+|#\\d|\\?|!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter se trata de una de las fuentes más comunes a la hora de tratar datos para el procesado de lenguaje natural. La clase **nltk.tokenize.TweetTokenizer** nos aporta una serie de métodos y atributos extra a la hora de parsear tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "#Nos creamos nuestra lista de tweets\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "#Nos creamos una expresión regular que nos permita detectar hastags\n",
    "pattern = r'#\\w+'\n",
    "print(regexp_tokenize(tweets[0], pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp']\n"
     ]
    }
   ],
   "source": [
    "#Nos creamos una expresión regular que nos permita encontrar menciones\n",
    "pattern = r'([@]\\w+)'\n",
    "print(regexp_tokenize(tweets[-1], pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "#Ahora hacemos uso de TweetTokenizer, para ello en primer lugar nos creamos una instancia de dicho tipo \n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "#AHora hacemos uso de del método tokenize() para tokenizar cada uno de nuestro tweets \n",
    "print([tknzr.tokenize(t) for t in tweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a proceder a hacer uso de tekenizaciones avanzadas que nos van a permitir tokenizar strings/documentos que no esten en format ASCII, por ejemplo, texto en alemán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n"
     ]
    }
   ],
   "source": [
    "#Nos creamos nuestro string\n",
    "my_string = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
    "\n",
    "#Hacemos uso de word_tokenize para tokenizar nuestra string\n",
    "print(word_tokenize(my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Über']\n"
     ]
    }
   ],
   "source": [
    "#Ahora hacemos uso de una expresión que nos permita extraer aquellas palabras que empiezan por mayúsculas\n",
    "print(regexp_tokenize(my_string, r'[A-ZÜ]\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "#Ahora procedemos a extraer los emojis, para ello necesitamos el rango unicode de los emojis\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(my_string, emoji))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
