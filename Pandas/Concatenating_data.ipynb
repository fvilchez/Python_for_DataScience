{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "La función append nos permite concatenar objetos de tipo Series. Cuando hacemos esto, los índices de las distintas series concatenadas se mantienen. Si queremos resetear los índices podemos hacer uso de **reset_index(drop = True)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "#Importamos los datos\n",
    "jan = pd.read_csv('sales-jan-2015.csv', index_col = 'Date', parse_dates = True)\n",
    "feb = pd.read_csv('sales-feb-2015.csv', index_col = 'Date', parse_dates = True)\n",
    "mar = pd.read_csv('sales-mar-2015.csv', index_col = 'Date', parse_dates = True)\n",
    "\n",
    "#Extraemos de cada una de los series la columna Units\n",
    "jan_units = jan.loc[:, 'Units']\n",
    "feb_units = feb.loc[:, 'Units']\n",
    "mar_units = mar.loc[:, 'Units']\n",
    "\n",
    "#Concatenamos los valores \n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "#Extraemos info entre el 27 de enero y el 2 de febrero\n",
    "print(quarter1.loc['jan 27, 2015': 'feb 2, 2015'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642\n"
     ]
    }
   ],
   "source": [
    "#Hacemos la suma de valores\n",
    "print(quarter1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating pandas Series along row axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de **append()** podemos hacer uso de la función **concat()**. Esta función es mucho más versatil y potente que la función **append()**, ya que esta función nos permite concatenar tanto a nivel de filas como de columnas. El método **concat()**, dispone también del argumento **ignore_index = True**, que nos permite evitar que se mantengan los índices de las series originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Hacemos uso de concat a nivel de fila\n",
    "quarter1 = pd.concat([jan_units, feb_units, mar_units], axis = 'rows')\n",
    "\n",
    "#Extraemos info entre el 27 de enero y el 2 de febrero\n",
    "print(quarter1.loc['jan 27, 2015': 'feb 2, 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending DataFrames with ignore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método **append()** también puede ser usado para objetos de tipo DataFrame, esta también tiene el argumento **ignore_index = True** que nos permite crear un nuevo index y no conservar los índices de los DataFrames originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1935, 4)\n",
      "(19455, 4)\n",
      "(21390, 4)\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos \n",
    "names_1881 = pd.read_csv('names1881.csv')\n",
    "names_1981 = pd.read_csv('names1981.csv')\n",
    "\n",
    "#Agregamos la columna year a cada uno de nuestros datasets\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "#Combinamos \n",
    "names = names_1881.append(names_1981, ignore_index = True)\n",
    "\n",
    "#Vemos las dimensiones de nuestros datasets\n",
    "print(names_1881.shape)\n",
    "print(names_1981.shape)\n",
    "print(names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>Morgan</td>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>Morgan</td>\n",
       "      <td>F</td>\n",
       "      <td>1769</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14390</th>\n",
       "      <td>Morgan</td>\n",
       "      <td>M</td>\n",
       "      <td>766</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name gender  count  year\n",
       "1283   Morgan      M     23  1881\n",
       "2096   Morgan      F   1769  1981\n",
       "14390  Morgan      M    766  1981"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostramos todas las filas que tienen por nombre Morgan\n",
    "names.query('name == \"Morgan\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating pandas DataFrames along column axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo que deseamos es concatenar nuestros conjuntos de datos por columnas en lugar de por filas, debemos de hacer uso del método **concat()**, este método contiene el argumento axis que nos permite elegir el tipo de concatenación que deseamos realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max TemperatureF</th>\n",
       "      <th>Mean TemperatureF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Apr</th>\n",
       "      <td>89.0</td>\n",
       "      <td>53.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aug</th>\n",
       "      <td>NaN</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jan</th>\n",
       "      <td>68.0</td>\n",
       "      <td>32.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jul</th>\n",
       "      <td>91.0</td>\n",
       "      <td>72.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jun</th>\n",
       "      <td>NaN</td>\n",
       "      <td>70.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>NaN</td>\n",
       "      <td>62.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nov</th>\n",
       "      <td>NaN</td>\n",
       "      <td>39.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oct</th>\n",
       "      <td>84.0</td>\n",
       "      <td>55.451613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sep</th>\n",
       "      <td>NaN</td>\n",
       "      <td>63.766667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Max TemperatureF                    Mean TemperatureF                 \n",
       "Apr                                89.0                           53.100000\n",
       "Aug                                 NaN                           70.000000\n",
       "Dec                                 NaN                           34.935484\n",
       "Feb                                 NaN                           28.714286\n",
       "Jan                                68.0                           32.354839\n",
       "Jul                                91.0                           72.870968\n",
       "Jun                                 NaN                           70.133333\n",
       "Mar                                 NaN                           35.000000\n",
       "May                                 NaN                           62.612903\n",
       "Nov                                 NaN                           39.800000\n",
       "Oct                                84.0                           55.451613\n",
       "Sep                                 NaN                           63.766667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cargamos los datos\n",
    "weather_max = pd.read_csv('weather_max.csv', index_col = 'Month')\n",
    "weather_mean = pd.read_csv('weather_mean.csv', index_col = 'Month')\n",
    "\n",
    "#Concatenamos por columna\n",
    "weather_concat = pd.concat([weather_max, weather_mean], axis = 1)\n",
    "\n",
    "#Vemos el resultado\n",
    "weather_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading multiple files to build a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen situaciones en las cuales para construir nuestra dataframe definitivo tendremos que parsear varias conjuntos de datos y concatenarlos para crearnos nuestro DataFrame final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  gold  silver  bronze\n",
      "France             NaN   461.0   475.0\n",
      "Germany          407.0     NaN   454.0\n",
      "Italy            460.0   394.0     NaN\n",
      "Soviet Union     838.0   627.0   584.0\n",
      "United Kingdom   498.0   591.0   505.0\n",
      "United States   2088.0  1195.0  1052.0\n"
     ]
    }
   ],
   "source": [
    "medal_type = ['gold', 'silver', 'bronze']\n",
    "medals = []\n",
    "for medal in medal_type:\n",
    "    #Nos creamos el nombre del fichero del que queremos realizar la lectura\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    #Nos creamos la lista de columnas que deseamos parsear\n",
    "    columns = ['Country', medal]\n",
    "    #Leemos nuestro conjunto de datos\n",
    "    df = pd.read_csv(file_name, index_col = 'Country', header = 0, names = columns)\n",
    "    #Nos creamos una lista de dataframes\n",
    "    medals.append(df)\n",
    "\n",
    "medals = pd.concat(medals, axis = 'columns')\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Concatenating vertically to get MultiIndexed rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando apilamos múltiples DataFrames de forma vertical en un solo DataFrame, es deseable construir un MultiÍndice para indicar de donde provienen cada uno de los conjuntos de datos apilados. Esto se puede realizar haciendo uso del parámetro **keys** en **pd.concat()**, esto nos genera un índice jerárquico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Total\n",
      "       Country               \n",
      "bronze United States   1052.0\n",
      "       Soviet Union     584.0\n",
      "       United Kingdom   505.0\n",
      "       France           475.0\n",
      "       Germany          454.0\n",
      "silver United States   1195.0\n",
      "       Soviet Union     627.0\n",
      "       United Kingdom   591.0\n",
      "       France           461.0\n",
      "       Italy            394.0\n",
      "gold   United States   2088.0\n",
      "       Soviet Union     838.0\n",
      "       United Kingdom   498.0\n",
      "       Italy            460.0\n",
      "       Germany          407.0\n"
     ]
    }
   ],
   "source": [
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "medals = []\n",
    "for medal in medal_types:\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    medal_df = pd.read_csv(file_name, index_col = 'Country')\n",
    "    medals.append(medal_df)\n",
    "\n",
    "medals = pd.concat(medals, keys = medal_types)\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing MultiIndexed DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a proceder a ordenar nuestro conjunto de datos medals y haremos uso de **pd.IndexSlice** para extraer información específica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total    454.0\n",
      "Name: (bronze, Germany), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Ordenamos por el primer nivel\n",
    "medals_sorted = medals.sort_index(level = 0)\n",
    "\n",
    "#Vemos el número de medallas de bronce que gano Alemania\n",
    "print(medals_sorted.loc[('bronze', 'Germany')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Total\n",
      "Country               \n",
      "United States   1195.0\n",
      "Soviet Union     627.0\n",
      "United Kingdom   591.0\n",
      "France           461.0\n",
      "Italy            394.0\n"
     ]
    }
   ],
   "source": [
    "#Mostramos la información solo respecto a las medallas de plata\n",
    "print(medals_sorted.loc['silver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Total\n",
      "       Country              \n",
      "bronze United Kingdom  505.0\n",
      "gold   United Kingdom  498.0\n",
      "silver United Kingdom  591.0\n"
     ]
    }
   ],
   "source": [
    "#Nos creamos un IndexSlice\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "#Mostramos todas las medallas ganadas por UK\n",
    "print(medals_sorted.loc[idx[:, 'United Kingdom'], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating horizontally to get MultiIndexed columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible construir un DataFrame de forma jerárquica a partir de columnas. Para esto podemos hacer uso del argumento **axis** de **pd.concat()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20 entries, 2015-02-02 08:33:01 to 2015-02-26 08:58:51\n",
      "Data columns (total 9 columns):\n",
      "(Software, Company)    9 non-null object\n",
      "(Software, Product)    9 non-null object\n",
      "(Software, Units)      9 non-null float64\n",
      "(Service, Company)     6 non-null object\n",
      "(Service, Product)     6 non-null object\n",
      "(Service, Units)       6 non-null float64\n",
      "(Hardware, Company)    5 non-null object\n",
      "(Hardware, Product)    5 non-null object\n",
      "(Hardware, Units)      5 non-null float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 1.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Hacemos la lectura de los datos\n",
    "sales = []\n",
    "for file in glob('feb-sales*.csv'):\n",
    "    df = pd.read_csv(file, index_col = 'Date')\n",
    "    sales.append(df)\n",
    "    \n",
    "#Procedemos a concatenar todo en un dataframe\n",
    "february_sales = pd.concat(sales, keys = ['Software', 'Service', 'Hardware'], axis = 1)\n",
    "print(february_sales.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Software Service         Hardware\n",
      "                             Company Company          Company\n",
      "2015-02-02 08:33:01            Hooli     NaN              NaN\n",
      "2015-02-02 20:54:49              NaN     NaN        Mediacore\n",
      "2015-02-03 14:14:18          Initech     NaN              NaN\n",
      "2015-02-04 15:36:29        Streeplex     NaN              NaN\n",
      "2015-02-04 21:52:45              NaN     NaN  Acme Coporation\n",
      "2015-02-05 01:53:06  Acme Coporation     NaN              NaN\n",
      "2015-02-05 22:05:03              NaN   Hooli              NaN\n",
      "2015-02-07 22:58:10              NaN     NaN  Acme Coporation\n"
     ]
    }
   ],
   "source": [
    "#Nos creamos un elemento de tipo IndexSlice\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "#Seleccionamos la información del 2 de febrero al 8 de febrero de la columna Company\n",
    "print(february_sales.loc['2015-02-02':'2015-02-08', idx[:,'Company']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating DataFrames from a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Units\n",
      "         Company               \n",
      "february Acme Coporation     34\n",
      "         Hooli               30\n",
      "         Initech             30\n",
      "         Mediacore           45\n",
      "         Streeplex           37\n",
      "january  Acme Coporation     76\n",
      "         Hooli               70\n",
      "         Initech             37\n",
      "         Mediacore           15\n",
      "         Streeplex           50\n",
      "march    Acme Coporation      5\n",
      "         Hooli               37\n",
      "         Initech             68\n",
      "         Mediacore           68\n",
      "         Streeplex           40\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos\n",
    "jan = pd.read_csv('sales-jan-2015.csv')\n",
    "feb = pd.read_csv('sales-feb-2015.csv')\n",
    "mar = pd.read_csv('sales-mar-2015.csv')\n",
    "\n",
    "#Nos creamos una lista de tuplas\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "#Nos creamos un diccionario vacio\n",
    "month_dict = {}\n",
    "\n",
    "#Procedemos a crearnos nuestro conjunto de datos\n",
    "for month_name, info in month_list:\n",
    "    month_dict[month_name] = info.groupby('Company').sum()\n",
    "\n",
    "sales = pd.concat(month_dict)\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Units\n",
      "         Company         \n",
      "february Mediacore     45\n",
      "january  Mediacore     15\n",
      "march    Mediacore     68\n"
     ]
    }
   ],
   "source": [
    "#Nos creamos un objeto de tipo IndexSlice\n",
    "idx = pd.IndexSlice\n",
    "#Obtemos todas las unidades vendidas por parte de Hooli para cada mes\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de concatenar DataFrames tenemos la opción de hacer uso de DataFrames, para ello **pd.concat()** dispone del argumento **join**, este argumento puede tomar varios valores a continuación veremos el uso del valor **inner**. Cuando hacemos un **inner join** entre dos conjuntos de datos lo que hacemos es realizar un cruce entre índices y nos quedamos con las observaciones que tienen dicho índice común. En caso de que el número de las columnas fuese diferente, entonces en los valores que un determinado no tuviese valor, se pondría el valor de **NaN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\n",
      "                 Total   Total   Total\n",
      "Country                               \n",
      "United States   1052.0  1195.0  2088.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos\n",
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "medals = []\n",
    "for medal in medal_types:\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    medal_df = pd.read_csv(file_name, index_col = 'Country')\n",
    "    medals.append(medal_df)\n",
    "\n",
    "#Hacemos el inner join \n",
    "print(pd.concat(medals, axis = 1, keys = medal_types, join = 'inner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso hemos realizado el inner join entre tres conjuntos de datos, en estos tres conjuntos solo los países: United States, Soviet Union y United Kingdom eran comunes por lo tanto tras realizar el inner solo nos quedamos con la información de estas tres observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling & concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a realizar una comparación del producto interior bruto de los países de China y USA. Los datos de China empiezan en el año 1967 siendo estos datos anuales, mientras que los de USA se tratan de datos desde el año 1947 y están dados de forma trimestral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               china       usa\n",
      "Year                          \n",
      "1970-12-31  0.546128  0.980397\n",
      "1980-12-31  1.072537  1.660540\n",
      "1990-12-31  0.892820  1.088953\n",
      "2000-12-31  2.357522  0.719980\n",
      "2010-12-31  4.011081  0.455009\n",
      "2020-12-31  3.789936  0.377506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: \n",
      ".resample() is now a deferred operation\n",
      "You called pct_change(...) on this deferred object which materialized it into a dataframe\n",
      "by implicitly taking the mean.  Use .resample(...).mean() instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/francisco/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: \n",
      ".resample() is now a deferred operation\n",
      "You called pct_change(...) on this deferred object which materialized it into a dataframe\n",
      "by implicitly taking the mean.  Use .resample(...).mean() instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos\n",
    "china = pd.read_csv('gdp_china.csv', parse_dates = True)\n",
    "china = pd.concat([pd.to_datetime(china.Year, format='%Y'), china.GDP], axis = 1)\n",
    "china.set_index('Year', inplace = True)\n",
    "china.columns = ['china']\n",
    "\n",
    "usa = pd.read_csv('gdp_usa.csv', parse_dates = True, index_col = 'DATE')\n",
    "usa.columns = ['usa']\n",
    "\n",
    "#Procedemos a hacer un resample y calcular el porcentaje de cambio con un offset de 10 años\n",
    "china_annual = china.resample('A').pct_change(10).dropna()\n",
    "usa_annual = usa.resample('A').pct_change(10).dropna()\n",
    "\n",
    "#Concatenamos los ambos conjuntos de datos\n",
    "df_final = pd.concat([china_annual, usa_annual], axis = 1, join = 'inner')\n",
    "\n",
    "#\n",
    "print(df_final.resample('10A').last())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
