{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación procederemos a echar un vistazo a siete algoritmos de regresión, los cuales nos permitirán realizar un breve chequeo de en que punto se encuentran los datos. Veremos cuatro algoritmos lineales y tres no lineales.\n",
    "\n",
    "* **Algoritmos Lineales:**\n",
    "  \n",
    "  * Linear Regression\n",
    "  \n",
    "  * Ridge Regression\n",
    "  \n",
    "  * Lasso Linear Regression\n",
    "  \n",
    "  * Elastic-Net Regression\n",
    "  \n",
    "\n",
    "* **Algoritmos no lineales:**\n",
    "\n",
    "  * K-Nearest Neighbors\n",
    "  \n",
    "  * Regression Trees\n",
    "  \n",
    "  * Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos Lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión lineal asume que las variables de entrada tienen una distribución Gaussiana. También asume que las variables de entrada son relevantes para la variable de salida y que estas no están altamente correladas entre ellas (problema conocido como colinearilidad).\n",
    "\n",
    "Podemos hacer uso del algoritmo de regresión lineal a partir de la clase **LinearRegression** perteneciente a la librería **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.84606709922741\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "file_name = 'boston.csv'\n",
    "names = ['CRIM' , 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "         'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "#Cargamos los datos\n",
    "df = pd.read_csv(file_name, delim_whitespace=True, names=names)\n",
    "array = df.values\n",
    "\n",
    "#Separamos entre predictores y variable a predecir\n",
    "X = array[:, 0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "#Hacemos la validación cruzada\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv = kfold, scoring = scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trata de una extensión de la regresión lineal. Es una técnica usada para analizar datos que sufren el problema de multicolinealidad. Cuando se produce este suceso, las estimaciones de mínimos cuadrados no son sesgadas, pero sus variaciones son grandes, por lo que pueden estar lejos del verdadero valor. Al agregar un grado de sesgo a las estimaciones, la regresión de ridge reduce los errores estándar y así conseguir unas estimaciones de las que nos podamos fiar más. Hace uso de la conocida como norma **L2**.\n",
    "\n",
    "Podemos construir una regresión de **Ridge** a partir de la clase **Ridge** de la librería **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.17876741942594\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "file_name = 'boston.csv'\n",
    "names = ['CRIM' , 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "         'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "#Cargamos los datos\n",
    "df = pd.read_csv(file_name, delim_whitespace=True, names=names)\n",
    "array = df.values\n",
    "\n",
    "#Separamos entre predictores y variable a predecir\n",
    "X = array[:, 0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "#Hacemos la validación cruzada\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = Ridge()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv = kfold, scoring = scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que la regresión de Ridge se trata de una extensión de la regresión lineal, que permite solucionar el problema de la multicolinealidad. En este caso hace uso de la norma **L1**. Lasso también nos permite realizar una selección de variables.\n",
    "\n",
    "Podemos hacer uso de la regresión de Lasso a partir de la clase **Lasso** de la librería **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.47903204687103\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "file_name = 'boston.csv'\n",
    "names = ['CRIM' , 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "         'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "#Cargamos los datos\n",
    "df = pd.read_csv(file_name, delim_whitespace=True, names=names)\n",
    "array = df.values\n",
    "\n",
    "#Separamos entre predictores y variable a predecir\n",
    "X = array[:, 0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "#Hacemos la validación cruzada\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = Lasso()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv = kfold, scoring = scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic-Net Regresssion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic-Net se trata de un forma de regularizar la regresión que combina las propiedades de las regresiones de Ridge y de Lasso. Se busca minimizar la complejidad del modelo de regresión, penalizando el modelo haciendo de las normas **L2 ** y  **L1**.\n",
    "\n",
    "Podemos hacer uso del modelo ElasticNet haciendo uso de la clase **ElasticNet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.17449996156528\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "file_name = 'boston.csv'\n",
    "names = ['CRIM' , 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "         'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "#Cargamos los datos\n",
    "df = pd.read_csv(file_name, delim_whitespace=True, names=names)\n",
    "array = df.values\n",
    "\n",
    "#Separamos entre predictores y variable a predecir\n",
    "X = array[:, 0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "#Hacemos la validación cruzada\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ElasticNet()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv = kfold, scoring = scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algoritmos no lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo KNN lo que trata es de localizar las k instancias más similares entre si en un conjunto de entrenamiento para una nueva instancia no vista. Se predice el valor de la variable a predecir para esta instancia como la media de las instancias asignadas como más similares. Para asignar las k instancias más similares el algoritmo KNN hace uso de una métrica basada en distancias. La distancia de Minkowski es usada por defecto, la cual es una generalización de la distancia Euclidea y de la distancia de Manhattan. \n",
    "\n",
    "Disponemos de la clase **KNeighborsRegressor** para construir nuestro regresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-107.91645930980391\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "filename = 'boston.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "         'B', 'LSTAT', 'MEDV' ]\n",
    "dataframe = pd.read_csv(filename, delim_whitespace=True, names=names)\n",
    "\n",
    "#Obtenemos las variables predictoras y las variables a predecir\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state = 7)\n",
    "model = KNeighborsRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring = scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de algoritmos hacen uso del conjunto de entrenamiento para seleccionar los mejores puntos para dividir nuestro conjunto de datos con el objetivo de minimizar el coste. Por defecto la métrica que trata de minimizar este algoritmo es el MSE, pero mediante el parámetro **criterion** le podemos especificar otra métrica.\n",
    "\n",
    "Podemos hacer uso de la clase **DecisionTreeRegressor** para crear nuestro algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40.19559568627451\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Cargamos los datos\n",
    "filename = 'boston.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "         'B', 'LSTAT', 'MEDV' ]\n",
    "dataframe = pd.read_csv(filename, delim_whitespace=True, names=names)\n",
    "\n",
    "#Separamos entre las variables predictoras y la variable a predecir\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "#Preparamos el kfold y aplicamos algoritmo \n",
    "kfold = KFold(n_splits = 10, random_state = 7)\n",
    "model = DecisionTreeRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv = kfold, scoring = scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo conocido como SVM está desarrollado para trabajar con problemas de clasificación binaria. Esta técnica se ha extendido para predecir valores reales haciendo uso de SVR. Podemos hacer uso de la clase SVR para implementar este algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.04717038292698\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#Cargamos los datos\n",
    "filename = 'boston.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "         'B', 'LSTAT', 'MEDV' ]\n",
    "dataframe = pd.read_csv(filename, delim_whitespace=True, names=names)\n",
    "\n",
    "#Separamos entre las variables predictoras y la variable a predecir\n",
    "array = dataframe.values\n",
    "X = array[:, 0:13]\n",
    "Y = array[:,13]\n",
    "\n",
    "#Preparamos el kfold y lanzamos el algoritmo\n",
    "kfold = KFold(n_splits = 10, random_state = 7)\n",
    "model = SVR()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv = kfold, scoring = scoring)\n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
