{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot-Checking es una forma de descubrir que algoritmos funcionan bien en nuestro problema de aprendizaje automático. No podemos saber aquellos algoritmos que son más adecuados a nuestro problema de antemano. Debemos probar un número de métodos y centrarnos en aquellos que den mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Spot-Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No podemos conocer de antemano que algoritmo funcionará mejor a la hora de resolver nuestro problema. Debemos hacer uso del método prueba y error para detectar una pequeña lista de algoritmos que funcionan bien y a partir de esta lista podemos afinar.\n",
    "\n",
    "Cuando queremos empezar a utilizar algoritmos la pregunta que nos debemos hacer es: ¿qué conjunto de algoritmos se adaptan mejor a nuestro problema? en lugar de cometer el error de preguntarnos: ¿qué algoritmo se adapta mejor a nuestro problema?. Lo mejor es probar un conjunto de algoritmos y ver cuál es que mejor resultados proporciona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de aprendizaje automático lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión logística asume distribuciones gaussianas para las variables de entrada numéricas, puede ser usado para modelar problemas de clasificación binarios. Podemos construir un modelo de regresión logística haciendo uso de la clase **LogisticRegression** de la librería sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7695146958304853\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos \n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class']\n",
    "df = pd.read_csv(filename, names=names)\n",
    "\n",
    "#Separamos entre variable a predecir y variables predictoras\n",
    "X = df.values[:, 0:8]\n",
    "Y = df.values[:,8]\n",
    "\n",
    "#Fijamos el kFold y lanzamos nuestro modelo\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Técnica estadística para clasificación binaria y multiclase. Asume distribuciones gaussianas para las variables numéricas de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064251538\n"
     ]
    }
   ],
   "source": [
    "#Cargamos los datos \n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class']\n",
    "df = pd.read_csv(filename, names=names)\n",
    "\n",
    "#Separamos entre variable a predecir y variables predictoras\n",
    "X = df.values[:, 0:8]\n",
    "Y = df.values[:,8]\n",
    "\n",
    "#FIjamos el kfold y lanzamos el modelo\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algoritmos no lineales de aprendizaje automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K vecinos más cercanos (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de k vecinos más cercanos (KNN) usa como métrica la distancia para encontrar las k instancias más similares en el conjunto de entrenamiento y a la hora de evaluar una nueva instancia toma el resultado promedio de los vecinos como predicción. Podemos hacer uso de este algoritmo mediante la clase **KNeighborsClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7265550239234451\n"
     ]
    }
   ],
   "source": [
    "#Carga de datos\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class']\n",
    "dataframe = pd.read_csv(filename, names=names)\n",
    "\n",
    "#Separamos entre predictores y variable a predecir\n",
    "X = dataframe.values[:, 0:8]\n",
    "Y = dataframe.values[:,8]\n",
    "\n",
    "#Preparamos un kfold y lanzamos algoritmo\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes calcula la probabilidad de cada clase y la probabilidad condicionada de cada clase dado un valor de entrada. Estas probabilidades son estimadas para una nueva observación y multiplicadas conjuntamente, asumiendo que todas son independientes. Cuando trabajamos con datos reales, se asume una distribución Gaussianna para estimar facilmente las probabilidades para las variables de entrada a partir de la PDF Gaussianna. Podemos hacer uso de este algoritmo mediante la clase **GaussianNB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7551777170198223\n"
     ]
    }
   ],
   "source": [
    "#Carga de datos\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class']\n",
    "dataframe = pd.read_csv(filename, names=names)\n",
    "\n",
    "#Separamos entre los predictores y la variable a predecir\n",
    "X = dataframe.values[:,0:8]\n",
    "Y = dataframe.values[:,8]\n",
    "\n",
    "#Preparamos nuestro kfold y lanzamos algoritmo \n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árboles de decisión (regresión y clasificación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles de regresíon y clasificación (CART) construyen un árbol binario a partir del conjunto de entrenamiento. Los puntos de corte son seleccionados evaluando cada valor de cada atributo con el objetivo de minimizar una determinada función de coste (por ejemplo, el índice de Gini). Podemos hacer uso de este modelo mediante la clase **DecisionTreeClassifier **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7017088174982911\n"
     ]
    }
   ],
   "source": [
    "#Carga de datos\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class']\n",
    "dataframe = pd.read_csv(filename, names=names)\n",
    "\n",
    "#Separamos entre los predictores y la variable a predecir\n",
    "X = dataframe.values[:, 0:8]\n",
    "Y = dataframe.values[:,8]\n",
    "\n",
    "#Preparamos kfold y lanzamos algoritmo \n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = DecisionTreeClassifier()\n",
    "results= cross_val_score(model, X, Y, cv = kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) busca la línea que mejor separa las dos clases. Las instancias que están más cerca de la línea que mejor separa las clases se denominan vectores de soporte y son los que influyen en la ubicación de la línea. SVM ha si expandido a múltiples clases. De particular importancia es el uso de múltiplos kernels a partir del argumento **kernel**. Podemos hacer uso de este algoritmo median la clase de sklearn **SVC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510252904989747\n"
     ]
    }
   ],
   "source": [
    "#Carga de datos\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class']\n",
    "dataframe = pd.read_csv(filename, names=names)\n",
    "\n",
    "#Separamos en predictores y variable a predecir\n",
    "X = dataframe.values[:,0:8]\n",
    "Y = dataframe.values[:,8]\n",
    "\n",
    "#Preparamos kfold y lanzamos algoritmo\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = SVC()\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
